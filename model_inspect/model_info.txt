================================================================================
MODEL CONFIGURATION
================================================================================
Model type: distilbert
Architecture: ['DistilBertForSequenceClassification']
Number of labels: 3
Hidden size: 768
Number of layers: 6
Attention heads: 12
Max position embeddings: 512
Vocab size: 30522

LABEL MAPPING
--------------------------------------------------------------------------------
ID 0: ERROR
ID 1: WARNING
ID 2: INFO

Label to ID:
ERROR: 0
INFO: 2
WARNING: 1

TOKENIZER INFORMATION
--------------------------------------------------------------------------------
Tokenizer class: DistilBertTokenizerFast
Vocab size: 30522
Model max length: 512
Special tokens:
  PAD:  [PAD] (ID 0)
  UNK:  [UNK] (ID 100)
  CLS:  [CLS] (ID 101)
  SEP:  [SEP] (ID 102)
  MASK: [MASK] (ID 103)

MODEL ARCHITECTURE
--------------------------------------------------------------------------------
DistilBertForSequenceClassification(
  (distilbert): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0-5): 6 x TransformerBlock(
          (attention): DistilBertSdpaAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
            (activation): GELUActivation()
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)
  (classifier): Linear(in_features=768, out_features=3, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)

PARAMETER STATISTICS
--------------------------------------------------------------------------------
Total parameters: 66,955,779
Trainable parameters: 66,955,779
Non-trainable parameters: 0

FILES IN MODEL DIRECTORY
--------------------------------------------------------------------------------
config.json                                    0.00 MB
model.safetensors                            255.43 MB
special_tokens_map.json                        0.00 MB
tokenizer.json                                 0.68 MB
tokenizer_config.json                          0.00 MB
vocab.txt                                      0.22 MB

INFERENCE TEST
--------------------------------------------------------------------------------
Input IDs shape: torch.Size([1, 7])
Attention mask shape: torch.Size([1, 7])
Logits shape: torch.Size([1, 3])
Logits: tensor([[ 1.1761, -1.0952,  0.0419]])
Probabilities: tensor([[0.7018, 0.0724, 0.2258]])